{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca81b72c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009205,
     "end_time": "2022-12-11T23:15:56.089280",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.080075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook contains the complete, end to end pipeline for training and successfully predicting on test!\n",
    "\n",
    "Here are the things we'll cover.\n",
    "\n",
    "1. [Training a fast.ai model](#section-one)\n",
    "2. [Using the model to predict on test](#section-two)\n",
    "3. [Making a submission](#section-three)\n",
    "\n",
    "For training, we will use images preprocessed to PNGs that I shared here: [RSNA Mammo PNGs 256px, 384px, 512px, 768px, 1024px](https://www.kaggle.com/datasets/radek1/rsna-mammography-images-as-pngs)\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547753d",
   "metadata": {
    "papermill": {
     "duration": 0.005031,
     "end_time": "2022-12-11T23:15:56.102834",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.097803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Other resources you might find useful:\n",
    "\n",
    "* [üí° how to process DICOM images to PNGs](https://www.kaggle.com/code/radek1/how-to-process-dicom-images-to-pngs)\n",
    "* [üìä EDA + training a fast.ai model + submission üöÄ](https://www.kaggle.com/code/radek1/eda-training-a-fast-ai-model-submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f7170",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004781,
     "end_time": "2022-12-11T23:15:56.112889",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.108108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Versions\n",
    "\n",
    "* **v10**: res18, 4 epochs trained in submission, 256px, 3 splits, `CV: 0.046/0.076 LB: 0.05`, unfortunately had a bug in best thresh calulation\n",
    "* **v11**: res18, 4 epochs trained in submission, 256px, 4 splits, ‚è∞ times out\n",
    "* **v15**: res18, 4 epochs trained externally, 256px, CPU (the processing of images might be the most expensive part of inference! especially with resnet18, 2 CPUs -> 4 CPUs might be the way to go)\n",
    "* **v16**: res18, 4 epochs trained externally, 256px, CPU, higher thresh (the means of predictions should be better than predictions from an individual model, a higher threshold *might* work better)\n",
    "* **v17**: res18, like `v15`, but trained with cherry picking, either performing well in general (doubt it) or overfitted to local CV\n",
    "* **v18**: tf_efficientnetv2_s, 512x512, single model\n",
    "* **v19**: tf_efficientnetv2_s, 512x512, ensemble of 4 models\n",
    "* **v20**: tf_efficientnetv2_s, like v19, but running with different pretrained weights, main difference -- this one runs on the CPU\n",
    "* **v22**: like v18, but taking max instead of mean of predictions on images, that makes more sense but who knows if it will work better in practice\n",
    "* **v23**: moving to new arch and adding VOI LUT transformation (might make the pipeline time out as it makes the processing a bit longer)\n",
    "* **v24**: like v23 but with mean of individual thresholds recorded during training\n",
    "* **v25**: like v23 but without VOI LUT processing\n",
    "* **v26**: a single model with particularly good optimized pfbeta1 of just under 0.196\n",
    "* **v27**: a single model with an optimized pfbeta1 of 0.212\n",
    "* **v28**: a single model with an optimized pfbeta1 of 0.249\n",
    "* **v30**: like v28, but with mean aggregation of predictions by `prediction_id`\n",
    "* **v31**: a single model with an optimized pfbeta1 of 0.252\n",
    "* **v32**: a single model with an optimized pfbeta1 of 0.207 trained on 1024x1024\n",
    "* **v33**: similar to v32, but trained with augmentation and results in much higher threshold (plus locally the difference between pfbeta and optimized pfbeta is much greater)\n",
    "* **v34**: like v32, but trained for one more epoch and on a different fold\n",
    "* **v35**: an ensemble of v32 and v34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a14d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004797,
     "end_time": "2022-12-11T23:15:56.122584",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.117787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Updates (account of working on this -- documents the despair of an ML practitioner we so often encounter!)\n",
    "\n",
    "**12/04**: I find myself in the spot so often encountered by DL practitioners -- the model simply doesn't train! Or rather, it trains poorly to the point where one cannot be certain to what extent it is making good use of the available information.\n",
    "\n",
    "This is an interesting case as a lot can be happening. From what I observed on the forums (special thanks go to Theo Viel and his awesome notebook: [RSNA Breast Baseline - Inference](https://www.kaggle.com/code/theoviel/rsna-breast-baseline-inference), now there is also [this notebook](https://www.kaggle.com/code/hengck23/notebooke04a738685) by hengck23), people are feeding the data to their model in a way that I wouldn't expect. Theo is only normalizing it to between 0 and 1.\n",
    "\n",
    "But this might mean that some important information to the model is lost in the processing that I do here. Or it could be a hunderd of other small things the community has arrived at through so many RSNA competitions üôÇ.\n",
    "\n",
    "Three things to do:\n",
    "\n",
    "* explore various ways of phrsing the problem within `fastai` (training with Sigmoid, Softmax, etc)\n",
    "* get the pipeline on Kaggle working with what I have\n",
    "    * to always have a working pipeline version (otherwise it is very easy to lose momentum on a project)\n",
    "    * to be verifying if local improvements generalize to LB (so far, I have a very weak handle on what is going on in this competition, the more data I collect the better)\n",
    "* after the above, see what happens if I feed data to my model either how Theo or hengck23 does it (normalized to [0,1], zero-centered and standard dev normalized)\n",
    "\n",
    "**12/05**: I stayed up for more than half the night on Saturday and Sunday to work on this as I knew I wouldn't get much time to give this over the week. There can be an all-consuming quality to DL projects if you are not careful.\n",
    "\n",
    "I reimplemented the entire pipeline in pure PyTorch... and it doesn't train in the SAME WAY as my fast.ai pipeline...\n",
    "\n",
    "```\n",
    "step:     1    val_loss: 0.686    pf1: 0.042    auc: 0.464\n",
    "step:   500    val_loss: 0.108    pf1: 0.023    auc: 0.479\n",
    "step:  1000    val_loss: 0.106    pf1: 0.023    auc: 0.579\n",
    "step:  1500    val_loss: 0.105    pf1: 0.021    auc: 0.558\n",
    "step:  2000    val_loss: 0.107    pf1: 0.020    auc: 0.524\n",
    "step:  2500    val_loss: 0.106    pf1: 0.024    auc: 0.599\n",
    "step:  3000    val_loss: 0.105    pf1: 0.023    auc: 0.575\n",
    "step:  3500    val_loss: 0.104    pf1: 0.025    auc: 0.610\n",
    "step:  4000    val_loss: 0.103    pf1: 0.027    auc: 0.623\n",
    "step:  4500    val_loss: 0.104    pf1: 0.027    auc: 0.612\n",
    "step:  5000    val_loss: 0.104    pf1: 0.027    auc: 0.607\n",
    "step:  5125    val_loss: 0.104    pf1: 0.027    auc: 0.606\n",
    "CPU times: user 12min 15s, sys: 38 s, total: 12min 53s\n",
    "Wall time: 12min 56s\n",
    "```\n",
    "\n",
    "I replaced the training loop, how data is read, the data I am reading, added a metric from `torchmetrics` for another bit of sanity check and also did a couple of things that don't make sense but you essentially question everything when stuff doesn't train. The things I tried that didn't make much sense was training on larger images (why 512px should work if 256px doesn't train reasonably?) and training with sigmoid and BCE vs softmax and nll (with two classes they should be equivalent). Oh yeah, I also tried different models of course, directly from fast.ai, the one from Theo, pretrained, not pretrained, etc.\n",
    "\n",
    "AND YET THERE ARE PEOPLE ON THE LB whose models do seem to train, potentially with ease.\n",
    "\n",
    "What is the secret? What am I missing?\n",
    "\n",
    "The only low hanging fruit I can think of now is addressing the class imbalance... Counting by images, there are roughly 2% of positive examples in the dataset. Maybe this can make a difference.\n",
    "\n",
    "Other than that, the only 3 components I haven't tried replacing so far are:\n",
    "* the researcher working on this (me)\n",
    "* the splitting functionality into train and val\n",
    "* my computer\n",
    "* *maybe* the dataset, as in, would my code train on cats and dogs?\n",
    "\n",
    "**12/05**: I think I found a way to train the model so that it works üò≠üò≠üò≠üò≠üò≠ Don't want to jinx it though üôÇ Let me move it over to the Kaggle pipeline and let us see how it fares here üôÇ Writing [the thoughts down in this thread](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/370587) was really helpful üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3c74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T00:49:16.208915Z",
     "iopub.status.busy": "2022-12-02T00:49:16.208242Z",
     "iopub.status.idle": "2022-12-02T00:49:16.247884Z",
     "shell.execute_reply": "2022-12-02T00:49:16.244677Z",
     "shell.execute_reply.started": "2022-12-02T00:49:16.208760Z"
    },
    "papermill": {
     "duration": 0.004797,
     "end_time": "2022-12-11T23:15:56.132313",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.127516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-one\"></a>\n",
    "# Training a fast.ai model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9514855a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:15:56.144901Z",
     "iopub.status.busy": "2022-12-11T23:15:56.143808Z",
     "iopub.status.idle": "2022-12-11T23:17:09.078614Z",
     "shell.execute_reply": "2022-12-11T23:17:09.077475Z"
    },
    "papermill": {
     "duration": 72.944243,
     "end_time": "2022-12-11T23:17:09.081513",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.137270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: timm-with-dependencies\r\n",
      "Processing ./timm-with-dependencies/timm-0.6.12-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.1)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (21.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.64.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.28.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.13.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.7.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.8.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2022.9.24)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.6.12\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/dicomsdl-offline-installer/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Installing collected packages: dicomsdl\r\n",
      "Successfully installed dicomsdl-0.109.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!unzip -q ../input/timm-with-dependencies/timm_all -d timm-with-dependencies\n",
    "!pip install --no-index --find-links timm-with-dependencies timm\n",
    "!pip install /kaggle/input/dicomsdl-offline-installer/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "\n",
    "from fastai.vision.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.metrics import ActivationType\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a552b6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:09.094824Z",
     "iopub.status.busy": "2022-12-11T23:17:09.094366Z",
     "iopub.status.idle": "2022-12-11T23:17:12.010025Z",
     "shell.execute_reply": "2022-12-11T23:17:12.009035Z"
    },
    "papermill": {
     "duration": 2.925095,
     "end_time": "2022-12-11T23:17:12.012470",
     "exception": false,
     "start_time": "2022-12-11T23:17:09.087375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "NUM_SPLITS = 4\n",
    "\n",
    "RESIZE_TO = (1024, 1024)\n",
    "\n",
    "DATA_PATH = '/kaggle/input/rsna-breast-cancer-detection'\n",
    "TRAIN_IMAGE_DIR = '/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_512'\n",
    "TEST_DICOM_DIR = '/kaggle/input/rsna-breast-cancer-detection/test_images'\n",
    "MODEL_PATH = '/kaggle/input/rsna-trained-model-weights/tf_effv2_s_208_402/tf_effv2_s_208_402'\n",
    "\n",
    "label_smoothing_weights = torch.tensor([1,10]).float()\n",
    "if torch.cuda.is_available():\n",
    "    label_smoothing_weights = label_smoothing_weights.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcf2da",
   "metadata": {
    "papermill": {
     "duration": 0.005474,
     "end_time": "2022-12-11T23:17:12.023782",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.018308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating stratified splits for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3c5922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.036587Z",
     "iopub.status.busy": "2022-12-11T23:17:12.035741Z",
     "iopub.status.idle": "2022-12-11T23:17:12.167059Z",
     "shell.execute_reply": "2022-12-11T23:17:12.166062Z"
    },
    "papermill": {
     "duration": 0.140411,
     "end_time": "2022-12-11T23:17:12.169705",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.029294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
    "patient_id_any_cancer = train_csv.groupby('patient_id').cancer.max().reset_index()\n",
    "skf = StratifiedKFold(NUM_SPLITS, shuffle=True, random_state=42)\n",
    "splits = list(skf.split(patient_id_any_cancer.patient_id, patient_id_any_cancer.cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf007ad",
   "metadata": {
    "papermill": {
     "duration": 0.005815,
     "end_time": "2022-12-11T23:17:12.182599",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.176784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cced5b",
   "metadata": {
    "papermill": {
     "duration": 0.005317,
     "end_time": "2022-12-11T23:17:12.193513",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.188196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am defining some functionality here to make our life easier and to get us the last 10% of the way to a really good result.\n",
    "\n",
    "Generally, none of this code is core to training or predicting, we could skip most of it and still be able to get a well trained model.\n",
    "\n",
    "\n",
    "But here we want to push the boundaries of performance so let's get these things in üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd8b65d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.206337Z",
     "iopub.status.busy": "2022-12-11T23:17:12.206034Z",
     "iopub.status.idle": "2022-12-11T23:17:12.267726Z",
     "shell.execute_reply": "2022-12-11T23:17:12.266812Z"
    },
    "papermill": {
     "duration": 0.070613,
     "end_time": "2022-12-11T23:17:12.269689",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.199076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/369267  \n",
    "def pfbeta_torch(preds, labels, beta=1):\n",
    "    if preds.dim() != 2 or (preds.dim() == 2 and preds.shape[1] !=2): raise ValueError('Houston, we got a problem')\n",
    "    preds = preds[:, 1]\n",
    "    preds = preds.clip(0, 1)\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/369886    \n",
    "def pfbeta_torch_thresh(preds, labels):\n",
    "    optimized_preds = optimize_preds(preds, labels)\n",
    "    return pfbeta_torch(optimized_preds, labels)\n",
    "\n",
    "def optimize_preds(preds, labels=None, thresh=None, return_thresh=False, print_results=False):\n",
    "    preds = preds.clone()\n",
    "    if labels is not None: without_thresh = pfbeta_torch(preds, labels)\n",
    "    \n",
    "    if not thresh and labels is not None:\n",
    "        threshs = np.linspace(0, 1, 101)\n",
    "        f1s = [pfbeta_torch((preds > thr).float(), labels) for thr in threshs]\n",
    "        idx = np.argmax(f1s)\n",
    "        thresh, best_pfbeta = threshs[idx], f1s[idx]\n",
    "\n",
    "    preds = (preds > thresh).float()\n",
    "\n",
    "    if print_results:\n",
    "        print(f'without optimization: {without_thresh}')\n",
    "        pfbeta = pfbeta_torch(preds, labels)\n",
    "        print(f'with optimization: {pfbeta}')\n",
    "        print(f'best_thresh = {thresh}')\n",
    "    if return_thresh:\n",
    "        return thresh\n",
    "    return preds\n",
    "\n",
    "fn2label = {fn: cancer_or_not for fn, cancer_or_not in zip(train_csv['image_id'].astype('str'), train_csv['cancer'])}\n",
    "\n",
    "def splitting_func(paths):\n",
    "    train = []\n",
    "    valid = []\n",
    "    for idx, path in enumerate(paths):\n",
    "        if int(path.parent.name) in patient_id_any_cancer.iloc[splits[SPLIT][0]].patient_id.values:\n",
    "            train.append(idx)\n",
    "        else:\n",
    "            valid.append(idx)\n",
    "    return train, valid\n",
    "\n",
    "def label_func(path):\n",
    "    return fn2label[path.stem]\n",
    "\n",
    "def get_items(image_dir_path):\n",
    "    items = []\n",
    "    for p in get_image_files(image_dir_path):\n",
    "        items.append(p)\n",
    "        if p.stem in fn2label and int(p.parent.name) in patient_id_any_cancer.iloc[splits[SPLIT][0]].patient_id.values:\n",
    "            if label_func(p) == 1:\n",
    "                for _ in range(5):\n",
    "                    items.append(p)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6fab9",
   "metadata": {
    "papermill": {
     "duration": 0.005327,
     "end_time": "2022-12-11T23:17:12.280715",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.275388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wrapping getting data and getting a model into functions -- this way our logic for training will be cleaner to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9305afdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.293066Z",
     "iopub.status.busy": "2022-12-11T23:17:12.292764Z",
     "iopub.status.idle": "2022-12-11T23:17:12.300332Z",
     "shell.execute_reply": "2022-12-11T23:17:12.299343Z"
    },
    "papermill": {
     "duration": 0.016358,
     "end_time": "2022-12-11T23:17:12.302564",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.286206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models.layers.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "from torch.nn import Flatten\n",
    "\n",
    "def get_dataloaders():\n",
    "    train_image_path = TRAIN_IMAGE_DIR\n",
    "\n",
    "    dblock = DataBlock(\n",
    "        blocks    = (ImageBlock, CategoryBlock),\n",
    "        get_items = get_items,\n",
    "        get_y = label_func,\n",
    "        splitter  = splitting_func,\n",
    "        batch_tfms=[Flip()],\n",
    "    )\n",
    "    dsets = dblock.datasets(train_image_path)\n",
    "    return dblock.dataloaders(train_image_path, batch_size=32)\n",
    "\n",
    "def get_learner(arch=resnet18):\n",
    "    learner = vision_learner(\n",
    "        get_dataloaders(),\n",
    "        arch,\n",
    "        custom_head=nn.Sequential(SelectAdaptivePool2d(pool_type='avg', flatten=Flatten()), nn.Linear(1280, 2)),\n",
    "        metrics=[\n",
    "            error_rate,\n",
    "            AccumMetric(pfbeta_torch, activation=ActivationType.Softmax, flatten=False),\n",
    "            AccumMetric(pfbeta_torch_thresh, activation=ActivationType.Softmax, flatten=False)\n",
    "        ],\n",
    "        loss_func=CrossEntropyLossFlat(weight=torch.tensor([1,50]).float()),\n",
    "        pretrained=True,\n",
    "        normalize=False\n",
    "    ).to_fp16()\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c5143",
   "metadata": {
    "papermill": {
     "duration": 0.005319,
     "end_time": "2022-12-11T23:17:12.313292",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.307973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the learner and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fca3e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.325277Z",
     "iopub.status.busy": "2022-12-11T23:17:12.325016Z",
     "iopub.status.idle": "2022-12-11T23:19:02.450923Z",
     "shell.execute_reply": "2022-12-11T23:19:02.449587Z"
    },
    "papermill": {
     "duration": 110.135069,
     "end_time": "2022-12-11T23:19:02.453784",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.318715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rsna-2022-whl/pydicom-2.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/pylibjpeg-1.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg==1.4.0) (1.21.6)\r\n",
      "pydicom is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Installing collected packages: python-gdcm, pylibjpeg\r\n",
      "Successfully installed pylibjpeg-1.4.0 python-gdcm-3.0.15\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/rsna-2022-whl/torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1) (4.1.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (2.28.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (9.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (1.26.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (2022.9.24)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (2.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (3.3)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.11.0\r\n",
      "    Uninstalling torch-1.11.0:\r\n",
      "      Successfully uninstalled torch-1.11.0\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.12.0\r\n",
      "    Uninstalling torchvision-0.12.0:\r\n",
      "      Successfully uninstalled torchvision-0.12.0\r\n",
      "Successfully installed torch-1.12.1 torchvision-0.13.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# This is a dependency that is needed for reading DICOM images\n",
    "\n",
    "try:\n",
    "    import pylibjpeg\n",
    "except:\n",
    "    !rm -rf /root/.cache/torch/hub/checkpoints/\n",
    "    !mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "    !pip install /kaggle/input/rsna-2022-whl/{pydicom-2.3.0-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n",
    "    !pip install /kaggle/input/rsna-2022-whl/{torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl,torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl}\n",
    "\n",
    "# copying the pretrained weights\n",
    "\n",
    "if not os.path.exists('/root/.cache/torch/hub/checkpoints/'):\n",
    "        os.makedirs('/root/.cache/torch/hub/checkpoints/')\n",
    "!cp '/kaggle/input/pretrained-model-weights-for-fastai/resnet18-f37072fd.pth' '/root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth'\n",
    "!cp '/kaggle/input/pretrained-model-weights-for-fastai/tf_efficientnetv2_s-eb54923e.pth' '/root/.cache/torch/hub/checkpoints/tf_efficientnetv2_s-eb54923e.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45def01d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:19:02.469086Z",
     "iopub.status.busy": "2022-12-11T23:19:02.468744Z",
     "iopub.status.idle": "2022-12-11T23:21:06.522875Z",
     "shell.execute_reply": "2022-12-11T23:21:06.521668Z"
    },
    "papermill": {
     "duration": 124.064883,
     "end_time": "2022-12-11T23:21:06.525622",
     "exception": false,
     "start_time": "2022-12-11T23:19:02.460739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 2 s, total: 1min 28s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds, labels = [], []\n",
    "\n",
    "SPLIT = 0 # our learner needs this to construct its dataloaders...\n",
    "learn = get_learner('tf_efficientnetv2_s')\n",
    "\n",
    "# instead of training, to conserve pipeline time, I am uploading models trained locally\n",
    "# uncomment the lines below for training\n",
    "  \n",
    "# for SPLIT in range(NUM_SPLITS):\n",
    "#     learn = get_learner()\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit_one_cycle(NUM_EPOCHS, 1e-4, pct_start=0.1)\n",
    "#     learn.save(f'{MODEL_PATH}/{SPLIT}')\n",
    "        \n",
    "#     output = learn.get_preds()\n",
    "#     preds.append(output[0])\n",
    "#     labels.append(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c802fd20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:06.546885Z",
     "iopub.status.busy": "2022-12-11T23:21:06.546537Z",
     "iopub.status.idle": "2022-12-11T23:21:06.551315Z",
     "shell.execute_reply": "2022-12-11T23:21:06.550361Z"
    },
    "papermill": {
     "duration": 0.020109,
     "end_time": "2022-12-11T23:21:06.555793",
     "exception": false,
     "start_time": "2022-12-11T23:21:06.535684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# threshold = optimize_preds(torch.cat(preds), torch.cat(labels), return_thresh=True, print_results=True)\n",
    "threshold = 0.402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae5369",
   "metadata": {
    "papermill": {
     "duration": 0.009508,
     "end_time": "2022-12-11T23:21:06.575219",
     "exception": false,
     "start_time": "2022-12-11T23:21:06.565711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predicting on test<a id=\"section-two\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5319e4ff",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:06.596836Z",
     "iopub.status.busy": "2022-12-11T23:21:06.596500Z",
     "iopub.status.idle": "2022-12-11T23:21:11.542626Z",
     "shell.execute_reply": "2022-12-11T23:21:11.541306Z"
    },
    "papermill": {
     "duration": 4.959966,
     "end_time": "2022-12-11T23:21:11.545524",
     "exception": false,
     "start_time": "2022-12-11T23:21:06.585558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pydicom\n",
    "# from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import dicomsdl\n",
    "    \n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "import cv2\n",
    "\n",
    "!rm -rf test_resized_{RESIZE_TO[0]}\n",
    "\n",
    "def dicom_file_to_ary(path):\n",
    "    dcm_file = dicomsdl.open(str(path))\n",
    "    data = dcm_file.pixelData()\n",
    "\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    if dcm_file.getPixelDataInfo()['PhotometricInterpretation'] == \"MONOCHROME1\":\n",
    "        data = 1 - data\n",
    "\n",
    "    data = cv2.resize(data, RESIZE_TO)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "directories = list(Path(TEST_DICOM_DIR).iterdir())\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    parent_directory = str(directory_path).split('/')[-1]\n",
    "    !mkdir -p test_resized_{RESIZE_TO[0]}/{parent_directory}\n",
    "    for image_path in directory_path.iterdir():\n",
    "        processed_ary = dicom_file_to_ary(image_path)\n",
    "        cv2.imwrite(\n",
    "            f'test_resized_{RESIZE_TO[0]}/{parent_directory}/{image_path.stem}.png',\n",
    "            processed_ary\n",
    "        )\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as p:\n",
    "    p.map(process_directory, directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06661e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:11.562063Z",
     "iopub.status.busy": "2022-12-11T23:21:11.560974Z",
     "iopub.status.idle": "2022-12-11T23:21:30.953165Z",
     "shell.execute_reply": "2022-12-11T23:21:30.951702Z"
    },
    "papermill": {
     "duration": 19.402972,
     "end_time": "2022-12-11T23:21:30.955539",
     "exception": false,
     "start_time": "2022-12-11T23:21:11.552567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.61 s, sys: 1.68 s, total: 5.29 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds_all = []\n",
    "\n",
    "test_dl = learn.dls.test_dl(get_image_files(f'test_resized_{RESIZE_TO[0]}'))\n",
    "for SPLIT in range(NUM_SPLITS):\n",
    "    learn.load(f'{MODEL_PATH}/{SPLIT}')\n",
    "    preds, _ = learn.get_preds(dl=test_dl)\n",
    "    preds_all.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f27e90c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:30.972430Z",
     "iopub.status.busy": "2022-12-11T23:21:30.972123Z",
     "iopub.status.idle": "2022-12-11T23:21:30.980760Z",
     "shell.execute_reply": "2022-12-11T23:21:30.979127Z"
    },
    "papermill": {
     "duration": 0.019482,
     "end_time": "2022-12-11T23:21:30.982952",
     "exception": false,
     "start_time": "2022-12-11T23:21:30.963470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.zeros_like(preds_all[0])\n",
    "for pred in preds_all:\n",
    "    preds += pred\n",
    "\n",
    "preds /= NUM_SPLITS\n",
    "\n",
    "\n",
    "preds = optimize_preds(preds, thresh=threshold)\n",
    "image_ids = [path.stem for path in test_dl.items]\n",
    "\n",
    "image_id2pred = defaultdict(lambda: 0)\n",
    "for image_id, pred in zip(image_ids, preds[:, 1]):\n",
    "    image_id2pred[int(image_id)] = pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c1a4f",
   "metadata": {
    "papermill": {
     "duration": 0.007116,
     "end_time": "2022-12-11T23:21:30.997643",
     "exception": false,
     "start_time": "2022-12-11T23:21:30.990527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-three\"></a>\n",
    "# Making a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94742f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:31.014774Z",
     "iopub.status.busy": "2022-12-11T23:21:31.013177Z",
     "iopub.status.idle": "2022-12-11T23:21:31.057843Z",
     "shell.execute_reply": "2022-12-11T23:21:31.056560Z"
    },
    "papermill": {
     "duration": 0.057453,
     "end_time": "2022-12-11T23:21:31.062316",
     "exception": false,
     "start_time": "2022-12-11T23:21:31.004863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_id</th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10008_L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10008_R</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prediction_id  cancer\n",
       "0       10008_L     0.0\n",
       "1       10008_R     0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\n",
    "\n",
    "prediction_ids = []\n",
    "preds = []\n",
    "\n",
    "for _, row in test_csv.iterrows():\n",
    "    prediction_ids.append(row.prediction_id)\n",
    "    preds.append(image_id2pred[row.image_id])\n",
    "\n",
    "submission = pd.DataFrame(data={'prediction_id': prediction_ids, 'cancer': preds}).groupby('prediction_id').max().reset_index()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af61ec96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:31.095793Z",
     "iopub.status.busy": "2022-12-11T23:21:31.095143Z",
     "iopub.status.idle": "2022-12-11T23:21:31.113112Z",
     "shell.execute_reply": "2022-12-11T23:21:31.107743Z"
    },
    "papermill": {
     "duration": 0.036385,
     "end_time": "2022-12-11T23:21:31.116858",
     "exception": false,
     "start_time": "2022-12-11T23:21:31.080473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9bf46c",
   "metadata": {
    "papermill": {
     "duration": 0.010281,
     "end_time": "2022-12-11T23:21:31.138116",
     "exception": false,
     "start_time": "2022-12-11T23:21:31.127835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And that's it! Thank you very much for reading! üôÇ\n",
    "\n",
    "**If you enjoyed the notebook, please upvote! üôè Thank you, appreciate your support!**\n",
    "\n",
    "Happy Kaggling ü•≥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 345.439384,
   "end_time": "2022-12-11T23:21:33.927684",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-11T23:15:48.488300",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
